'''
Data Processing Module
'''
import vectors as vec
import matrix as mat
import statistics as stat
import gradientDescent as gd
import random

# Function used to split data into fractions [prob], [1 - prob]
def splitData(data, prob):
    results = [], []
    for row in data:
        results[0 if random.random() < prob else 1].append(row)
    
    return results

# Retrieve training and testing data and labels based on data split pct
def trainTestSplit(x, y, testPct):
    data = zip(x,y)
    train, test = splitData(data, 1 - testPct)
    x_train, y_train = zip(*train)
    x_test, y_test = zip(*test)

    return x_train, x_test, y_train, y_test

# Given multiple dimensions in input data (nxk matrix) 
# Get correlation between each feature
def correlationMatrix(data):
    
    _, num_cols = mat.shape(data)

    def correlationMatrixElement(i, j):
        # get the correlation betwen i and j dimensions
        return stat.correlation(mat.getColumn(data, i), mat.getColumn(data, j))

    return mat.createMatrix(num_cols, num_cols, correlationMatrixElement)


# Rescale data so mean of 0 and st dev of 1
def scale(data):
    num_rows, num_cols = mat.shape(data)
    
    # get means and standard deviations of every vector in given data matrix
    means = [stat.mean(mat.getColumn(data, j)) for j in range(num_cols)]
    stdevs = [stat.standardDeviation(mat.getColumn(data, j)) for j in range(num_cols)]

    return means, stdevs


def rescale(data):
    
    # get means and stdevs of data
    means, stdevs = scale(data)

    def rescaled(i, j):
        if stdevs[j] > 0:
            return (data[i][j] - means[j]) / stdevs[j]
        else:
            return data[i][j]
    
    num_rows, num_cols = mat.shape(data)

    # create and return new scaled matrix
    return mat.createMatrix(num_rows, num_cols, rescaled)


# Dimensionality Reduction

# Principal Component Analysis to extract one or more dimensions that
# capture as much of the variation in the data as possible
def deMeanMatrix(A):

    # return matrix with every column of mean 0
    nr, nc = mat.shape(A)
    col_means, _ = scale(A)

    return mat.createMatrix(nr, nc, lambda i, j: A[i][j] - col_means[j])


def direction(w):
    mag = vec.magnitude(w)
    return [w_i / mag for w_i in w]


# Compute the variance of our data in the direction  of W
def directionalVariance_(x_i, w):
    return vec.dot(x_i, direction(w)) ** 2

def directionalVariance(X, w):
    return sum(directionalVariance_(x_i, w) for x_i in X)


# Find the direction that maximizes this variance
def directionalVarianceGradient_(x_i, w):
    projection_length = vec.dot(x_i, direction(w))
    return [2 * projection_length * x_ij for x_ij in x_i]

def directionalVarianceGradient(X, w):
    return vec.vectorSum(directionalVarianceGradient_(x_i, w) for x_i in X)


# First Principle Component is the direction that maximizes the directionalVarianceGradient
def firstPrincipalComponentSGD(X):
    guess = [1 for _ in X[0]]

    unscaled_maximizer = gd.maximizeStochastic(
        lambda x, _, w: directionalVariance_(x, w),
        lambda x, _, w: directionalVarianceGradient_(x, w),
        X,
        [None for _ in X],
        guess
    )

    return direction(unscaled_maximizer)

# We project our data onto the direction generated by FPC
def project(v, w):
    # project v onto w
    projection_length = vec.dot(v, w)
    return vec.scalarMultiply(projection_length, w)

# To find more components, remove the projection from the data
def removeProjectionFromVector(v, w):
    return vec.vectorSub(v, project(v, w))

def removeProjection(X, w):
    return [removeProjectionFromVector(x_i, w) for x_i in X]

# We can iteratively find as many components as we want in higher dimension data
def principalComponentAnalysis(X, num_comps):
    components = []

    for _ in range(num_comps):
        component = firstPrincipalComponentSGD(X)
        components.append(component)

        # remove the components from out data
        X = removeProjection(X, component)
    
    return components


# Compute model performance statistics
def accuracy(tp, fp, tn, fn):
    correct = tp + tn
    total = tp + fp + tn + fn
    return correct / total

# Measure how accurate our positive predicitons are
def precision(tp, fp, tn, fn):
    return tp / (tp + fp)

# Measure what fraction of the positives our model identified
def recall(tp, fp, tn, fn):
    return tp / (tp + tn)

# f1 Score which combined both precision and recall
def f1_score(tp, fp, tn, fn):
    p = precision(tp, fp, tn, fn)
    r = recall(tp, fp, tn, fn)
    return 2 * p * r / (p + r)